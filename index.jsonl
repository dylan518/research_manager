{"role": "user", "content": "use semantic scholr api to do research on data free RL in LLMs"}
{"type": "function_call", "name": "python", "call_id": "call_CzgRjDBBZbKMj0wSfPPXUvn7", "arguments": "{\"code\":\"import os, textwrap, json, re, math, pandas as pd, requests\\n\\ndef get_env(k):\\n    return os.environ.get(k)\\n\\nS2_KEY = get_env('S2_KEY')\\nS2_KEY\\n\"}"}
{"type": "function_call_output", "call_id": "call_CzgRjDBBZbKMj0wSfPPXUvn7", "output": "{\"ok\": true, \"stdout\": \"\", \"result\": \"7rRLR7kFVV1ZaVXLaEeeg6l8EBXGe4Uk4gCtDA2r\"}"}
{"type": "function_call", "name": "python", "call_id": "call_a6CPcNcN2Hx1k1HBteSsv28M", "arguments": "{\"code\":\"import requests, urllib.parse, pandas as pd\\n\\nheaders = {'x-api-key': S2_KEY}\\n\\nquery = 'data-free reinforcement learning large language model'\\nurl = 'https://api.semanticscholar.org/graph/v1/paper/search'\\nparams = {\\n    'query': query,\\n    'limit': 20,\\n    'fields': 'title,year,authors,venue,abstract,url,citationCount,openAccessPdf,externalIds'\\n}\\nresp = requests.get(url, params=params, headers=headers, timeout=30)\\nresp.status_code, resp.text[:200]\\n\"}"}
{"type": "function_call_output", "call_id": "call_a6CPcNcN2Hx1k1HBteSsv28M", "output": "{\"ok\": true, \"stdout\": \"\", \"result\": [200, \"{\\\"total\\\": 45519, \\\"offset\\\": 0, \\\"next\\\": 20, \\\"data\\\": [{\\\"paperId\\\": \\\"9974199b138c09aadef80de8194a25110814daa8\\\", \\\"externalIds\\\": {\\\"ArXiv\\\": \\\"2511.12908\\\", \\\"DBLP\\\": \\\"journals/corr/abs-2511-12908\\\", \\\"DOI\\\": \\\"10.485\"]}"}
{"type": "function_call", "name": "python", "call_id": "call_KpvO7WaRLChcPngIotpHdnYt", "arguments": "{\"code\":\"data = resp.json()['data']\\nlen(data), data[0].keys()\\n\"}"}
{"type": "function_call_output", "call_id": "call_KpvO7WaRLChcPngIotpHdnYt", "output": "{\"ok\": true, \"stdout\": \"\", \"result\": [20, \"dict_keys(['paperId', 'externalIds', 'url', 'title', 'venue', 'year', 'citationCount', 'openAccessPdf', 'authors', 'abstract'])\"]}"}
{"type": "function_call", "name": "python", "call_id": "call_jVvX825cbJ0WXWY9UQIVtPkb", "arguments": "{\"code\":\"papers = []\\nfor p in data:\\n    papers.append({\\n        'title': p.get('title'),\\n        'year': p.get('year'),\\n        'venue': p.get('venue'),\\n        'citationCount': p.get('citationCount'),\\n        'url': p.get('url'),\\n        'arxiv': (p.get('externalIds') or {}).get('ArXiv'),\\n        'doi': (p.get('externalIds') or {}).get('DOI'),\\n        'pdf': (p.get('openAccessPdf') or {}).get('url'),\\n        'authors': ', '.join([a['name'] for a in p.get('authors',[])[:6]]) + (' et al.' if len(p.get('authors',[]))>6 else ''),\\n        'abstract': p.get('abstract')\\n    })\\n\\ndf = pd.DataFrame(papers)\\ndf[['year','citationCount','title','arxiv','url']].head(10)\\n\"}"}
{"type": "function_call_output", "call_id": "call_jVvX825cbJ0WXWY9UQIVtPkb", "output": "{\"ok\": true, \"stdout\": \"\", \"result\": [\"year\", \"citationCount\", \"title\", \"arxiv\", \"url\"]}"}
{"type": "function_call", "name": "python", "call_id": "call_CFBg7wV4Rs1DZRH4BeDPbbAM", "arguments": "{\"code\":\"df[['year','citationCount','title','arxiv','url']].to_string(index=False)[:1500]\\n\"}"}
{"type": "function_call_output", "call_id": "call_CFBg7wV4Rs1DZRH4BeDPbbAM", "output": "{\"ok\": true, \"stdout\": \"\", \"result\": \" year  citationCount                                                                                                                          title      arxiv                                                                            url\\n 2025              1       DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning 2511.12908 https://www.semanticscholar.org/paper/9974199b138c09aadef80de8194a25110814daa8\\n 2025             54              Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models via Vision-Guided Reinforcement Learning 2503.18013 https://www.semanticscholar.org/paper/1d9430bae9b3ac93ad8e0955e2a5d57745a91ccf\\n 2025             21                                                              A Survey on the Optimization of Large Language Model-based Agents 2503.12434 https://www.semanticscholar.org/paper/2ec6caff88519e06a3148fe2890f1741b30ce623\\n 2025              4                                         NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning 2505.16022 https://www.semanticscholar.org/paper/c5f39bcc5c9923f81047c804fb21ea0f761eab1b\\n 2025              2                                          Efficient Reinforcement Learning for Large Language Models with Intrinsic Exploration 2511.00794 https://www.semanticscholar.org/paper/fa0db37e2a5ac701e12b86a36d64602ae35bc87a\\n 2025             17                                                    \"}"}
{"type": "function_call", "name": "python", "call_id": "call_jshRwRNU7fTgzxASdZED0cA1", "arguments": "{\"code\":\"# refine query to focus on data-free / without human data / synthetic preference\\nqueries = [\\n    'data-free RLHF large language model',\\n    'human-free alignment reinforcement learning language model',\\n    'offline-free reinforcement learning language model',\\n    'reinforcement learning without human feedback language model',\\n    'self reward reinforcement learning language model'\\n]\\n\\ndef search(q, limit=10):\\n    r = requests.get('https://api.semanticscholar.org/graph/v1/paper/search',\\n                     params={'query': q, 'limit': limit,\\n                             'fields':'title,year,authors,venue,abstract,url,citationCount,openAccessPdf,externalIds'},\\n                     headers=headers, timeout=30)\\n    r.raise_for_status()\\n    return r.json()['data']\\n\\nallp=[]\\nfor q in queries:\\n    allp += [(q,p) for p in search(q, limit=20)]\\nlen(allp)\\n\"}"}
{"type": "function_call_output", "call_id": "call_jshRwRNU7fTgzxASdZED0cA1", "output": "{\"ok\": true, \"stdout\": \"\", \"result\": 100}"}
{"type": "function_call", "name": "python", "call_id": "call_lH1jDkGeCiO7ZOpqbrVnwRQB", "arguments": "{\"code\":\"# dedup by paperId\\nseen=set(); merged=[]\\nfor q,p in allp:\\n    pid=p['paperId']\\n    if pid in seen: continue\\n    seen.add(pid)\\n    merged.append(p)\\nlen(merged)\\n\"}"}
{"type": "function_call_output", "call_id": "call_lH1jDkGeCiO7ZOpqbrVnwRQB", "output": "{\"ok\": true, \"stdout\": \"\", \"result\": 95}"}
{"type": "function_call", "name": "python", "call_id": "call_rU3tQiQfZDLeazwrZn9gWXJX", "arguments": "{\"code\":\"mdf=pd.DataFrame([{\\n    'paperId':p['paperId'],\\n    'title':p.get('title'),\\n    'year':p.get('year'),\\n    'citationCount':p.get('citationCount'),\\n    'arxiv':(p.get('externalIds') or {}).get('ArXiv'),\\n    'url':p.get('url'),\\n    'abstract':p.get('abstract')\\n} for p in merged])\\n# rank by citationCount then year\\nmdf_sorted=mdf.sort_values(['citationCount','year'], ascending=[False,False])\\nmdf_sorted.head(20)[['citationCount','year','title','arxiv','url']].to_string(index=False)[:2000]\\n\"}"}
{"type": "function_call_output", "call_id": "call_rU3tQiQfZDLeazwrZn9gWXJX", "output": "{\"ok\": true, \"stdout\": \"\", \"result\": \" citationCount   year                                                                                                     title      arxiv                                                                            url\\n          7193 2023.0                            Direct Preference Optimization: Your Language Model is Secretly a Reward Model 2305.18290 https://www.semanticscholar.org/paper/0d1c76d45afa012ded7ab741194baf142117c495\\n          5463 2025.0                                 DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning 2501.12948 https://www.semanticscholar.org/paper/2eed1fad9bbf887d4395de40f20144c4fafefd7f\\n          2055 2021.0                                        Decision Transformer: Reinforcement Learning via Sequence Modeling 2106.01345 https://www.semanticscholar.org/paper/c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500\\n          1181 2023.0                                                          BloombergGPT: A Large Language Model for Finance 2303.17564 https://www.semanticscholar.org/paper/83edcfbb206ddad38a971d605da09390604248ea\\n           804 2023.0                                OpenAssistant Conversations - Democratizing Large Language Model Alignment 2304.07327 https://www.semanticscholar.org/paper/ae736662f64d56f3ab1894fbd9c45f8f37251843\\n           487 2023.0                           RRHF: Rank Responses to Align Language Models with Human Feedback without tears 2304.05302 https://www.semanticscholar.org/paper/748698bd4387afd08594e0dc8150c2afa210d9ae\\n           464 2023.0                    StructGPT: A General Framework for Large Language Model to Reason over Structured Data 2305.09645 https://www.semanticscholar.org/paper/e0f27336698c84709bd60b6b7f4ce588cbae66bf\\n           403 2025.0                                                                           Large Language Diffusion Models 2502.09992 https://www.semanticscholar.org/paper/0d11a9674b68216b92e08cf7617a93fbd3fb91f4\\n           324 2024.0                 \"}"}
{"type": "function_call", "name": "python", "call_id": "call_QdYRPQxeMJ7hafNcKTA2iehw", "arguments": "{\"code\":\"# pick more targeted via keyword filter in title/abstract\\nkw = ['data-free','human-free','self-reward','verifier','AI feedback','no human','synthetic','RLHF','RLAIF','DPO','RRHF','constitutional','self-alignment']\\n\\nimport numpy as np\\n\\ndef score_row(row):\\n    text = ((row['title'] or '') + ' ' + (row['abstract'] or '')).lower()\\n    return sum(1 for k in kw if k in text)\\n\\nmdf_sorted['kws']=mdf_sorted.apply(score_row, axis=1)\\nmdf_f=mdf_sorted.sort_values(['kws','year','citationCount'], ascending=[False,False,False])\\nmdf_f.head(25)[['kws','year','citationCount','title','arxiv','url']].to_string(index=False)[:2200]\\n\"}"}
{"type": "function_call_output", "call_id": "call_QdYRPQxeMJ7hafNcKTA2iehw", "output": "{\"ok\": true, \"stdout\": \"\", \"result\": \" kws   year  citationCount                                                                                                                    title      arxiv                                                                            url\\n   2 2025.0              8                                        Post-Training Large Language Models via Reinforcement Learning from Self-Feedback 2507.21931 https://www.semanticscholar.org/paper/d38c397873c4e93125a3be97ec645d01e9e4a1e1\\n   2 2025.0              4                                                             LaSeR: Reinforcement Learning with Last-Token Self-Rewarding 2510.14943 https://www.semanticscholar.org/paper/b51114590dc5ef1f896204dcc5d9d61dec26946a\\n   1 2025.0             54        Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models via Vision-Guided Reinforcement Learning 2503.18013 https://www.semanticscholar.org/paper/1d9430bae9b3ac93ad8e0955e2a5d57745a91ccf\\n   1 2025.0             45                                                         Self-Rewarding Vision-Language Model via Reasoning Decomposition 2508.19652 https://www.semanticscholar.org/paper/71bae1ffe7691f9164d1d38ef0090782051f4d71\\n   1 2025.0             18                                                   Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning 2505.24726 https://www.semanticscholar.org/paper/ee69f177c234ca34e1eaab8fcc3ae3c035e7e47f\\n   1 2025.0             13                                  Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning 2506.08745 https://www.semanticscholar.org/paper/f45e7ebf613d14bcfaf310b188b493cae42ec243\\n   1 2025.0              8                                        SPELL: Self-Play Reinforcement Learning for evolving Long-Context Language Models 2509.23863 https://www.semanticscholar.org/paper/27286e4d7612c5ccb55d3b47790269abde827314\\n   1 2025.0              4                                   NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning 2505.16022 https://www.semanticscholar.org/paper/c5f39bcc5c9923f81047c804fb21ea0f761eab1b\\n   1 2025.0              4    A Self-Supervised Reinforcem\"}"}
{"role": "assistant", "content": "Below is a **Semantic Scholar API\u2013grounded** mini literature scan on **data\u2011free RL for LLMs** (i.e., RL-style post-training/alignment *without relying on human-labeled preference datasets*, often using **self-feedback**, **AI feedback**, **verifiers**, **self-reward**, or **self-play**). I pulled these via the Semantic Scholar Graph API using queries like *\u201cdata-free RLHF large language model\u201d*, *\u201chuman-free alignment reinforcement learning language model\u201d*, *\u201cself reward reinforcement learning language model\u201d*, etc., and then filtered toward \u201chuman-free/self-reward/verifier\u201d concepts.\n\n## Most relevant recent papers (human-free / self-reward / verifier-free RL)\n1) **Post-Training Large Language Models via Reinforcement Learning from Self-Feedback** (2025)  \n   Semantic Scholar: https://www.semanticscholar.org/paper/d38c397873c4e93125a3be97ec645d01e9e4a1e1  \n   ArXiv: **2507.21931**  \n   Theme: RL using the model\u2019s own feedback signal (removes/limits human preference labels).\n\n2) **LaSeR: Reinforcement Learning with Last-Token Self-Rewarding** (2025)  \n   Semantic Scholar: https://www.semanticscholar.org/paper/b51114590dc5ef1f896204dcc5d9d61dec26946a  \n   ArXiv: **2510.14943**  \n   Theme: \u201cself-rewarding\u201d where reward is derived internally (e.g., from token-level signals).\n\n3) **Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning** (2025)  \n   Semantic Scholar: https://www.semanticscholar.org/paper/ee69f177c234ca34e1eaab8fcc3ae3c035e7e47f  \n   ArXiv: **2505.24726**  \n   Theme: iterative self-improvement loop (reflection + retries) coupled with RL updates.\n\n4) **Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning** (2025)  \n   Semantic Scholar: https://www.semanticscholar.org/paper/f45e7ebf613d14bcfaf310b188b493cae42ec243  \n   ArXiv: **2506.08745**  \n   Theme: use internal consistency/paths as a reward proxy for reasoning tasks.\n\n5) **NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning** (2025)  \n   Semantic Scholar: https://www.semanticscholar.org/paper/c5f39bcc5c9923f81047c804fb21ea0f761eab1b  \n   ArXiv: **2505.16022**  \n   Theme: RL where you *don\u2019t* need an external verifier model (aims to avoid extra labeled data / reward-modeling).\n\n6) **SPELL: Self-Play Reinforcement Learning for evolving Long-Context Language Models** (2025)  \n   Semantic Scholar: https://www.semanticscholar.org/paper/27286e4d7612c5ccb55d3b47790269abde827314  \n   ArXiv: **2509.23863**  \n   Theme: self-play/self-generated interaction data \u2192 RL updates without human preference datasets.\n\n## Adjacent / enabling work you\u2019ll see referenced a lot\nThese aren\u2019t always \u201cdata-free RL\u201d strictly, but they\u2019re commonly used as baselines or alternatives when discussing \u201cless human data\u201d post-training:\n\n- **Direct Preference Optimization (DPO)** (2023)  \n  Semantic Scholar: https://www.semanticscholar.org/paper/0d1c76d45afa012ded7ab741194baf142117c495  \n  ArXiv: **2305.18290**  \n  Note: still uses preference data, but avoids explicit reward modeling; often compared against RLHF / \u201chuman-free\u201d variants.\n\n- **RRHF: Rank Responses to Align Language Models with Human Feedback without tears** (2023)  \n  Semantic Scholar: https://www.semanticscholar.org/paper/748698bd4387afd08594e0dc8150c2afa210d9ae  \n  ArXiv: **2304.05302**  \n  Note: preference/ranking oriented; shows lighter-weight alignment pipelines.\n\n- **DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning** (2025)  \n  Semantic Scholar: https://www.semanticscholar.org/paper/2eed1fad9bbf887d4395de40f20144c4fafefd7f  \n  ArXiv: **2501.12948**  \n  Note: widely discussed in RL-for-reasoning; not necessarily \u201cdata-free\u201d end-to-end, but relevant to RL post-training trends.\n\n## How \u201cdata-free RL\u201d for LLMs is typically done (taxonomy you can use)\nWhen you read the above papers, they usually fit one (or more) of these patterns:\n\n1) **Self-reward / self-feedback**: the model generates a critique, score, or preference between candidates and uses it as reward.  \n2) **AI feedback (RLAIF-like)**: a separate LLM judge provides reward/preferences (reduces human labels, but not \u201cno data\u201d in the absolute sense).  \n3) **Verifier-based rewards**: reward from checkers (math verifier, unit tests, tool execution). Some papers try **verifier-free** variants.  \n4) **Self-play / synthetic interaction**: generate trajectories/dialogues/problems, then RL on that synthetic distribution.\n\n## If you want, I can run a tighter Semantic Scholar pass\nTell me which definition you mean by \u201cdata-free\u201d:\n\n- (A) **No human preference labels**, but AI judges/verifiers allowed  \n- (B) **No external judge/verifier**, only self-reward  \n- (C) **No task data at all** (purely prompt-driven synthetic generation)\n\n\u2026and whether you care more about **reasoning**, **instruction following**, or **tool use/agents**. I\u2019ll then pull a cleaner set (with abstracts, PDFs where available, and citation/venue info) and summarize the key methods + how they define reward."}
{"role": "user", "content": "im talking about the judge, challenger solver paradigm specifically"}
{"type": "function_call", "name": "python", "call_id": "call_Q6oqtwwpQMvBGHsTEZYGtJxf", "arguments": "{\"code\":\"import requests, pandas as pd, os, re\\nS2_KEY=os.environ.get('S2_KEY')\\nheaders={'x-api-key':S2_KEY}\\n\\ndef s2_search(query, limit=50, offset=0):\\n    url='https://api.semanticscholar.org/graph/v1/paper/search'\\n    params={'query':query,'limit':limit,'offset':offset,\\n            'fields':'title,year,authors,venue,abstract,url,citationCount,openAccessPdf,externalIds'}\\n    r=requests.get(url, params=params, headers=headers, timeout=30)\\n    r.raise_for_status()\\n    return r.json()\\n\\nqueries=[\\n    'judge challenger solver language model',\\n    'judge challenger solver reinforcement learning',\\n    'challenger model judge model solver',\\n    'LLM challenger judge',\\n    'self-play judge model large language model',\\n    'adversarial judge reward model language model'\\n]\\nresults=[]\\nfor q in queries:\\n    js=s2_search(q, limit=50)\\n    for p in js['data']:\\n        results.append((q,p))\\n\\n# dedup\\nseen=set(); papers=[]\\nfor q,p in results:\\n    if p['paperId'] in seen: continue\\n    seen.add(p['paperId'])\\n    papers.append(p)\\n\\ndf=pd.DataFrame([{\\n    'paperId':p['paperId'],\\n    'title':p.get('title',''),\\n    'year':p.get('year'),\\n    'citationCount':p.get('citationCount'),\\n    'arxiv':(p.get('externalIds') or {}).get('ArXiv'),\\n    'url':p.get('url'),\\n    'abstract':p.get('abstract','')\\n} for p in papers])\\n\\n# filter by keywords\\nkws=['judge','challenger','solver','adversarial','self-play','arena','debate','critique','verifier','reward model']\\n\\ndef kwscore(text):\\n    t=text.lower()\\n    return sum(1 for k in kws if k in t)\\n\\ndf['kws']=df.apply(lambda r: kwscore(r['title']+' '+r['abstract']), axis=1)\\n\\ndf_f=df.sort_values(['kws','year','citationCount'], ascending=[False,False,False])\\ndf_f.head(30)[['kws','year','citationCount','title','arxiv','url']]\\n\"}"}
{"type": "function_call_output", "call_id": "call_Q6oqtwwpQMvBGHsTEZYGtJxf", "output": "{\"ok\": false, \"stdout\": \"\", \"error\": \"429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=judge+challenger+solver+reinforcement+learning&limit=50&offset=0&fields=title%2Cyear%2Cauthors%2Cvenue%2Cabstract%2Curl%2CcitationCount%2CopenAccessPdf%2CexternalIds\"}"}
{"role": "assistant", "content": "The \u201c**judge\u2013challenger\u2013solver**\u201d paradigm is basically an **adversarial / self-play data-generation loop** where:\n\n- **Solver**: proposes answers/solutions/policies.\n- **Challenger**: proposes *hard* prompts/counterexamples/attacks that expose solver weaknesses.\n- **Judge**: scores solver outputs (or compares solver vs alternatives), producing a reward / preference signal.\n\nThis is one of the main \u201cdata-free\u201d alignment patterns because the **training signal comes from model-vs-model interaction + a judge**, rather than human-labeled preferences.\n\nI tried to pull a fresh targeted set from Semantic Scholar with those exact terms, but I hit a **Semantic Scholar API 429 rate limit** mid-query, so I can\u2019t reliably enumerate the exact paper list from the API response right now. If you want, I can retry in a bit (or you can share 5\u201310 candidate paper titles/authors and I\u2019ll resolve them precisely via the API once the limit clears).\n\nIn the meantime, here\u2019s how this paradigm shows up in the literature (useful for tightening your search terms), plus the *most common names* authors use for essentially the same setup.\n\n## What to search for (keywords that map to judge\u2013challenger\u2013solver)\nIn Semantic Scholar, \u201cjudge/challenger/solver\u201d as literal tokens is less common than these:\n\n- **self-play** (or *adversarial self-play*) + LLM / alignment / RL\n- **adversarial prompting** / **red-teaming** + *LLM-as-a-judge*\n- **debate** (two agents argue, judge decides)\n- **iterative refinement** with **critic/judge**\n- **generator\u2013discriminator** style training (not necessarily GANs; often \u201cadversarial data generation\u201d)\n- **RLAIF** (reinforcement learning from AI feedback) + *judge model*\n- **reward model as judge**, **pairwise preference judge**\n\nGood compound queries:\n- `(\"LLM-as-a-judge\" OR judge model) adversarial prompting self-play`\n- `debate judge language model reinforcement learning`\n- `adversarial data generation challenger solver judge`\n- `RLAIF judge model self-play`\n\n## Common concrete instantiations\n1) **Adversarial prompt generation**\n- Challenger searches for prompts where solver fails (jailbreaks, reasoning traps, factuality failures).\n- Judge scores solver response quality/safety.\n- Update solver (and sometimes challenger) to improve robustness.\n\n2) **Debate / argumentation**\n- Challenger and solver produce competing arguments/answers.\n- Judge chooses winner / assigns scalar reward.\n- Train solver to win under the judge.\n\n3) **Self-improvement via critiques**\n- Challenger = \u201ccritic\u201d that proposes counterexamples or critiques.\n- Judge = rubric-based evaluator (another LLM) producing reward/preferences.\n\n## Clarifying question (so I can re-run the API precisely)\nWhen you say \u201cjudge\u2013challenger\u2013solver,\u201d do you mean closer to:\n\nA) **LLM Debate**: solver vs challenger argue; judge picks winner  \nB) **Adversarial data generation / red-teaming**: challenger generates hardest prompts; judge scores solver  \nC) **Triplet training for reasoning**: challenger proposes alternative solutions; judge/verifier decides correctness  \nD) **Multi-agent RL** where *all three are trained* (not just solver)\n\nIf you pick A/B/C/D, I\u2019ll (1) re-query Semantic Scholar with the right synonyms, (2) return a curated bib (titles/authors/arXiv links), and (3) summarize the dominant algorithms (how challenger is optimized\u2014bandits, RL, search; how judge is calibrated; how solver is updated\u2014PPO/DPO-like/etc.)."}
{"role": "user", "content": "try requerying"}
{"type": "function_call", "name": "python", "call_id": "call_jGnAnj2xW6F1rsbk4giJvKPC", "arguments": "{\"code\":\"import time, requests, os, pandas as pd\\nS2_KEY=os.environ.get('S2_KEY')\\nheaders={'x-api-key':S2_KEY}\\n\\ndef s2_search(query, limit=20, offset=0, sleep=1.2):\\n    time.sleep(sleep)\\n    url='https://api.semanticscholar.org/graph/v1/paper/search'\\n    params={'query':query,'limit':limit,'offset':offset,\\n            'fields':'title,year,authors,venue,abstract,url,citationCount,openAccessPdf,externalIds'}\\n    r=requests.get(url, params=params, headers=headers, timeout=30)\\n    if r.status_code==429:\\n        return None, r\\n    r.raise_for_status()\\n    return r.json(), r\\n\\nqueries=[\\n    'LLM-as-a-judge adversarial prompting',\\n    'language model debate judge',\\n    'adversarial self-play large language model judge',\\n    'red teaming language model judge',\\n    'RLAIF judge model',\\n    'self-play reinforcement learning language model judge'\\n]\\n\\nall_p=[]\\nerrs=[]\\nfor q in queries:\\n    js, r = s2_search(q, limit=20, sleep=1.5)\\n    if js is None:\\n        errs.append((q, r.status_code, r.text[:120]))\\n        continue\\n    for p in js['data']:\\n        all_p.append((q,p))\\n\\nlen(all_p), errs[:2]\\n\"}"}
{"type": "function_call_output", "call_id": "call_jGnAnj2xW6F1rsbk4giJvKPC", "output": "{\"ok\": true, \"stdout\": \"\", \"result\": [120, []]}"}
{"type": "function_call", "name": "python", "call_id": "call_gYdbluumDJxbprHLVjy2uLSi", "arguments": "{\"code\":\"# dedup and rank by kw match\\nseen=set(); merged=[]\\nfor q,p in all_p:\\n    if p['paperId'] in seen: continue\\n    seen.add(p['paperId'])\\n    merged.append(p)\\n\\ndf=pd.DataFrame([{\\n    'paperId':p['paperId'],\\n    'title':p.get('title',''),\\n    'year':p.get('year'),\\n    'venue':p.get('venue'),\\n    'citationCount':p.get('citationCount'),\\n    'arxiv':(p.get('externalIds') or {}).get('ArXiv'),\\n    'doi':(p.get('externalIds') or {}).get('DOI'),\\n    'url':p.get('url'),\\n    'pdf':(p.get('openAccessPdf') or {}).get('url'),\\n    'abstract':p.get('abstract','')\\n} for p in merged])\\n\\nkws=['judge','debate','self-play','adversarial','challenger','red team','rlaif','ai feedback','preference','reward model','arena']\\n\\ndef kwscore(text):\\n    t=text.lower()\\n    return sum(1 for k in kws if k in t)\\n\\ndf['kws']=df.apply(lambda r: kwscore(r['title']+' '+r['abstract']), axis=1)\\n\\ndf_sorted=df.sort_values(['kws','citationCount','year'], ascending=[False,False,False])\\ndf_sorted.head(30)[['kws','citationCount','year','title','arxiv','url']].to_string(index=False)[:2500]\\n\"}"}
{"type": "function_call_output", "call_id": "call_gYdbluumDJxbprHLVjy2uLSi", "output": "{\"ok\": false, \"stdout\": \"\", \"error\": \"can only concatenate str (not \\\"NoneType\\\") to str\"}"}
{"type": "function_call", "name": "python", "call_id": "call_mY3oO5kxE72d4h0DI4TFBups", "arguments": "{\"code\":\"df['abstract']=df['abstract'].fillna('')\\ndf['title']=df['title'].fillna('')\\ndf['kws']=df.apply(lambda r: kwscore((r['title'] or '')+' '+(r['abstract'] or '')), axis=1)\\ndf_sorted=df.sort_values(['kws','citationCount','year'], ascending=[False,False,False])\\ndf_sorted.head(30)[['kws','citationCount','year','title','arxiv','url']].to_string(index=False)[:3000]\\n\"}"}
{"type": "function_call_output", "call_id": "call_mY3oO5kxE72d4h0DI4TFBups", "output": "{\"ok\": true, \"stdout\": \"\", \"result\": \" kws  citationCount   year                                                                                                                             title      arxiv                                                                            url\\n   5              0 2025.0 Oracle-RLAIF: An Improved Fine-Tuning Framework for Multi-modal Video Models through Reinforcement Learning from Ranking Feedback 2510.02561 https://www.semanticscholar.org/paper/5d86a3921c76f6933cf3debfd50ac5a70d1a5b4f\\n   4              0 2025.0                                             AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens 2512.17375 https://www.semanticscholar.org/paper/d07bf70999d4d93f25fe98c0ea939b767c54c466\\n   3           6998 2023.0                                                                            Judging LLM-as-a-judge with MT-Bench and Chatbot Arena 2306.05685 https://www.semanticscholar.org/paper/a0a79dad89857a96f8f71b14238e5237cbfc4787\\n   3            221 2024.0                                                                    Self-Play Preference Optimization for Language Model Alignment 2405.00675 https://www.semanticscholar.org/paper/df8c3a325419d63366b9b347739fcbf3e2c4d22c\\n   3            128 2024.0                                                                      Optimization-based Prompt Injection Attack to LLM-as-a-Judge 2403.17710 https://www.semanticscholar.org/paper/e56f14ced9f7ce344ed14bdcb46860ccac72ac83\\n   3             60 2024.0                                       MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image Generation? 2407.04842 https://www.semanticscholar.org/paper/bab59eabdb51a65dc3c16e43bf48a526ea6c0c9c\\n   3             60 2024.0                                                            RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness 2405.17220 https://www.semanticscholar.org/paper/db54861e0b31b0b341bbc9655f30dca98201ffca\\n   3             41 2024.0                                                      CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution 2410.16256 https://www.semanticscholar.org/paper/c1988c9aed878ae947efdbb7d7847b4418662a70\\n   3             34 2024.0                                                        Beyond Scalar Reward Model: Learning Generative Judge from Preference Data 2410.03742 https://www.semanticscholar.org/paper/158730a16cd4cc4d4842e42e5c2a0843b75d527c\\n   3             31 2025.0                                                                                                  One Token to Fool LLM-as-a-Judge 2507.08794 https://www.semanticscholar.org/paper/6478c185582f5e48c54677c683a9297ff2916118\\n   3             23 2025.0                                                            SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning 2504.19162 https://www.semanticscholar.org/paper/160820bb80922cfc3ff47a0c9b3a2bb369bb97b9\\n   3             11 2024.0          \"}"}
{"type": "function_call", "name": "python", "call_id": "call_TXd41cZaDfzqYagsUX64y5rj", "arguments": "{\"code\":\"df_sorted.head(80)[['kws','citationCount','year','title','arxiv','url']].to_string(index=False)\\n\"}"}
{"type": "function_call_output", "call_id": "call_TXd41cZaDfzqYagsUX64y5rj", "output": "{\"ok\": true, \"stdout\": \"\", \"result\": \" kws  citationCount   year                                                                                                                               title      arxiv                                                                            url\\n   5              0 2025.0   Oracle-RLAIF: An Improved Fine-Tuning Framework for Multi-modal Video Models through Reinforcement Learning from Ranking Feedback 2510.02561 https://www.semanticscholar.org/paper/5d86a3921c76f6933cf3debfd50ac5a70d1a5b4f\\n   4              0 2025.0                                               AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens 2512.17375 https://www.semanticscholar.org/paper/d07bf70999d4d93f25fe98c0ea939b767c54c466\\n   3           6998 2023.0                                                                              Judging LLM-as-a-judge with MT-Bench and Chatbot Arena 2306.05685 https://www.semanticscholar.org/paper/a0a79dad89857a96f8f71b14238e5237cbfc4787\\n   3            221 2024.0                                                                      Self-Play Preference Optimization for Language Model Alignment 2405.00675 https://www.semanticscholar.org/paper/df8c3a325419d63366b9b347739fcbf3e2c4d22c\\n   3            128 2024.0                                                                        Optimization-based Prompt Injection Attack to LLM-as-a-Judge 2403.17710 https://www.semanticscholar.org/paper/e56f14ced9f7ce344ed14bdcb46860ccac72ac83\\n   3             60 2024.0                                         MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image Generation? 2407.04842 https://www.semanticscholar.org/paper/bab59eabdb51a65dc3c16e43bf48a526ea6c0c9c\\n   3             60 2024.0                                                              RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness 2405.17220 https://www.semanticscholar.org/paper/db54861e0b31b0b341bbc9655f30dca98201ffca\\n   3             41 2024.0                                                        CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution 2410.16256 https://www.semanticscholar.org/paper/c1988c9aed878ae947efdbb7d7847b4418662a70\\n   3             34 2024.0                                                          Beyond Scalar Reward Model: Learning Generative Judge from Preference Data 2410.03742 https://www.semanticscholar.org/paper/158730a16cd4cc4d4842e42e5c2a0843b75d527c\\n   3             31 2025.0                                                                                                    One Token to Fool LLM-as-a-Judge 2507.08794 https://www.semanticscholar.org/paper/6478c185582f5e48c54677c683a9297ff2916118\\n   3             23 2025.0                                                              SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning 2504.19162 https://www.semanticscholar.org/paper/160820bb80922cfc3ff47a0c9b3a2bb369bb97b9\\n   3             11 2024.0                                      Self-Play with Adversarial Critic: Provable and Scalable Offline Alignment for Language Models 2406.04274 https://www.semanticscholar.org/paper/77f0687571a213c784f0901a821f22b2a03f3ddd\\n   3             10 2024.0                                                      Training Language Models to Win Debates with Self-Play Improves Judge Accuracy 2409.16636 https://www.semanticscholar.org/paper/1acc966b193be4eca4849d132ce3208a80d1c6fe\\n   3             10 2024.0                          The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation 2408.08688 https://www.semanticscholar.org/paper/e62740d1d5410ae5e08112b7e975f56169bf625a\\n   3              9 2025.0                                   LLMs Cannot Reliably Judge (Yet?): A Comprehensive Assessment on the Robustness of LLM-as-a-Judge 2506.09443 https://www.semanticscholar.org/paper/346affb6001b4f339d7e396d6aa3c6261c86072d\\n   3              7 2025.0     When Persuasion Overrides Truth in Multi-Agent LLM Debates: Introducing a Confidence-Weighted Persuasion Override Rate (CW-POR) 2504.00374 https://www.semanticscholar.org/paper/b4e979f4407cd3dd7fee0b8a87eab6b571f38514\\n   3              7 2024.0                   Debate, Deliberate, Decide (D3): A Cost-Aware Adversarial Framework for Reliable and Interpretable LLM Evaluation 2410.04663 https://www.semanticscholar.org/paper/ec1b0718bbd4561522f181758d77e205648f5f0f\\n   3              3 2025.0                                    Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks 2507.17747 https://www.semanticscholar.org/paper/a72cf9f7b9fe5ca7c8c784c9ef1ffdb36eced815\\n   3              2 2025.0                                                         ASP2LJ : An Adversarial Self-Play Laywer Augmented Legal Judgment Framework 2506.18768 https://www.semanticscholar.org/paper/d18eae5cd369a8e54ade619de3e1116c36912ca6\\n   3              2 2025.0                                                   Playpen: An Environment for Exploring Learning Through Conversational Interaction 2504.08590 https://www.semanticscholar.org/paper/cd005afc1a75b7df6770b4cd594688cc805160a8\\n   3              1 2025.0                                                                       CRAwDAD: Causal Reasoning Augmentation with Dual-Agent Debate 2511.22854 https://www.semanticscholar.org/paper/3d2cdb61dd203e5943dfc601b635698dc6967cc6\\n   3              0 2025.0                                                               Causal Judge Evaluation: Calibrated Surrogate Metrics for LLM Systems 2512.11150 https://www.semanticscholar.org/paper/5f62ff3089dc985179f961425bf1c8875cf55d75\\n   3              0 2025.0                                                                RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF 2510.14628 https://www.semanticscholar.org/paper/c9abd5b642786f16c0e3ac22c130a1e2d858e664\\n   3              0 2025.0                                                          Playpen: An Environment for Exploring Learning From Dialogue Game Feedback       None https://www.semanticscholar.org/paper/8bb847abfcc757c6f07d873c084e2be96fc38a73\\n   2            230 2023.0                                        Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback 2305.10142 https://www.semanticscholar.org/paper/2e6b6de08f459e2165b11ed8d2103916966b0fcf\\n   2            122 2023.0                                                               Explore, Establish, Exploit: Red Teaming Language Models from Scratch 2306.09442 https://www.semanticscholar.org/paper/1db819afb3604c4bfd1e5a0cb2ee9ab9dec52642\\n   2             75 2025.0                                                                       Preference Leakage: A Contamination Problem in LLM-as-a-judge 2502.01534 https://www.semanticscholar.org/paper/24e8029197b5f99126f0ef4e577d55a275405b36\\n   2             67 2023.0                                                                           Red Teaming Language Model Detectors with Language Models 2305.19713 https://www.semanticscholar.org/paper/fdb361ea83c010ed0011d179567de5a1112651ac\\n   2             43 2024.0                                                                                           Gradient-Based Language Model Red Teaming 2401.16656 https://www.semanticscholar.org/paper/409e0616a0fc02dd0ee8d5ae061944a98e9bd5a9\\n   2             35 2024.0                                  Towards Explainable Harmful Meme Detection through Multimodal Debate between Large Language Models 2401.13298 https://www.semanticscholar.org/paper/f3592207f424df697059455057f49ff1991a55e0\\n   2             27 2025.0                                                      Red teaming ChatGPT in medicine to yield real-world insights on model behavior       None https://www.semanticscholar.org/paper/8797299040bf410ba4d7371a974292e2afdbf205\\n   2             26 2024.0                                                                       Compromising Embodied Agents with Contextual Backdoor Attacks 2408.02882 https://www.semanticscholar.org/paper/8881ef166e51578c9c7654daf3c72f82ec16a339\\n   2             23 2022.0                                  Despite \\\"super-human\\\" performance, current LLMs are unsuited for decisions about ethics and safety 2212.06295 https://www.semanticscholar.org/paper/7d5175db1b99552491063d2d9581b0b51e1d2932\\n   2             21 2025.0 Benchmarking adversarial robustness to bias elicitation in large language models: scalable automated assessment with LLM-as-a-judge 2504.07887 https://www.semanticscholar.org/paper/a6db5ffa1a82b3d969f184b22e376ca04203b2dc\\n   2             19 2024.0                                                                          LLM-as-a-Judge & Reward Model: What They Can and Cannot Do 2409.11239 https://www.semanticscholar.org/paper/1158310f204db29513bd8322f7010f0ca78d69be\\n   2             17 2025.0                                       Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models 2506.07468 https://www.semanticscholar.org/paper/21251d14f97926bf4c19b8cfaab8a3d101f7fa71\\n   2             15 2025.0                                                            Compromising LLM Driven Embodied Agents With Contextual Backdoor Attacks       None https://www.semanticscholar.org/paper/dec60990afd52f480bb15d02240c840928b998b9\\n   2             12 2025.0                                                         Training an LLM-as-a-Judge Model: Pipeline, Insights, and Practical Lessons 2502.02988 https://www.semanticscholar.org/paper/3fe4155ad8598e2531159e459a07afcca5d43cec\\n   2             11 2025.0       EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic, Expressiveness, and Linguistic Challenges Using Model-as-a-Judge 2505.23009 https://www.semanticscholar.org/paper/d682d010d8c791070254f5cd97f3467441c31631\\n   2              9 2025.0                                                              Know Thy Judge: On the Robustness Meta-Evaluation of LLM Safety Judges 2503.04474 https://www.semanticscholar.org/paper/0ffb356aab98ae69c717f8b2969c3fed0592a048\\n   2              8 2025.0                                                                         Debate-Driven Multi-Agent LLMs for Phishing Email Detection 2503.22038 https://www.semanticscholar.org/paper/ad12c3fbc4791872683aff148fb2075c20ca0718\\n   2              8 2024.0                                   Efficient Self-Improvement in Multimodal Large Language Models: A Model-Level Judge-Free Approach 2411.17760 https://www.semanticscholar.org/paper/a2f44fc0f0c24fd4ab848f01a770a68dfa114f62\\n   2              7 2025.0                                         Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks 2505.13348 https://www.semanticscholar.org/paper/0c102fd337124c47205e9b79e5eb9e133de56ed4\\n   2              6 2025.0                                        Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment 2504.12663 https://www.semanticscholar.org/paper/a434fcdf7413bdc1fc5fade464a2bd453e12a1a3\\n   2              6 2024.0                           SPRec: Leveraging Self-Play to Debias Preference Alignment for Large Language Model-based Recommendations       None https://www.semanticscholar.org/paper/bfcb8b36fd3c73a8aa988856a80b1b151704c9c3\\n   2              5 2025.0                                                      Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections 2504.18333 https://www.semanticscholar.org/paper/502b573666af63c5baa7bc24673c9d4c78dd28e8\\n   2              5 2025.0                                                                      Can You Trick the Grader? Adversarial Persuasion of LLM Judges 2508.07805 https://www.semanticscholar.org/paper/d2a186f47b7977439fa5a930645ae06165af955f\\n   2              4 2025.0                                                                When AIs Judge AIs: The Rise of Agent-as-a-Judge Evaluation for LLMs 2508.02994 https://www.semanticscholar.org/paper/10734df8302641767f638c4a02fbf9e1cf08a666\\n   2              3 2025.0                                                                 LDGNet: LLMs Debate-Guided Network for Multimodal Sarcasm Detection       None https://www.semanticscholar.org/paper/853ef0476b8e26860de454f12c438973c2f5d2a0\\n   2              3 2025.0                                                              When to Trust Context: Self-Reflective Debates for Context Reliability 2506.06020 https://www.semanticscholar.org/paper/08b6e61a04d91d26ec06c8c8c51fa698f5e772f6\\n   2              3 2025.0                                                               Self-Refining Language Model Anonymizers via Adversarial Distillation 2506.01420 https://www.semanticscholar.org/paper/83d34e466f6fb3da89e52d45e9a8d01cd3ddda6e\\n   2              2 2025.0                                                     Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models 2507.02799 https://www.semanticscholar.org/paper/d4c4b2eb2cec8655191a7f4fa42a95aae1c980f3\\n   2              1 2025.0                                            Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak Experiments 2511.13788 https://www.semanticscholar.org/paper/33968a978ace858db44c91c43c2947dea245f767\\n   2              1 2025.0                                   TurQUaz at CheckThat! 2025: Debating Large Language Models for Scientific Web Discourse Detection 2508.08265 https://www.semanticscholar.org/paper/41d96afe6cfdb94091634c8fdb060b42b1b58540\\n   2              0 2026.0                                                                      Reasoning Model Is Superior LLM-Judge, Yet Suffers from Biases 2601.03630 https://www.semanticscholar.org/paper/cb84738f83b6b7339324b368d91edb579bf37dc0\\n   2              0 2026.0                     ExplainHM++: Explainable Harmful Meme Detection With Retrieval-Augmented Debate Between Large Multimodal Models       None https://www.semanticscholar.org/paper/ab83f170958e1d10c12fccfc311b7166fbd27cde\\n   2              0 2025.0                                                                         CourtGuard: A Local, Multiagent Prompt Injection Classifier 2510.19844 https://www.semanticscholar.org/paper/3329b815a0be506b5c0c8074214cbc8b1918aa91\\n   2              0 2025.0                               Between Generation and Judgment: A Cloud-Native Framework for Adversarial Evaluation of LLM Alignment       None https://www.semanticscholar.org/paper/cf968326de2af8eb2e088e5154f24bf6c1384ffb\\n   2              0 2025.0                                              RAG-targeted Adversarial Attack on LLM-based Threat Detection and Mitigation Framework 2511.06212 https://www.semanticscholar.org/paper/f56269f6b5e8ba4fa6befe20a24f3aa46d45769b\\n   2              0 2025.0                                                            The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction 2507.13732 https://www.semanticscholar.org/paper/a35ff5704666cc0c7597728745d61e5f86e48464\\n   2              0 2025.0                                                                                        SID: Multi-LLM Debate Driven by Self Signals 2510.06843 https://www.semanticscholar.org/paper/62642c0c76d84150906cc09b262b1d3288250bba\\n   2              0 2025.0                                              DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning 2512.07132 https://www.semanticscholar.org/paper/8ae6fd01fe68d02d1e121ed06e3ac0a16600f6b4\\n   2              0 2025.0                                       FindTheFlaws: Annotated Errors for Detecting Flawed Reasoning and Scalable Oversight Research 2503.22989 https://www.semanticscholar.org/paper/87362d2f42a4b72de95787763b896540b7e6d80a\\n   2              0 2025.0                                  A Survey on Large Reasoning Models with Self-Play Deep Reinforcement Learning and Chain-of-Thought       None https://www.semanticscholar.org/paper/3add9fb650640eb9fa2bc3ccdf53b2d7f25f0559\\n   2              0 2024.0               Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models 2409.16635 https://www.semanticscholar.org/paper/0bfa7c89253235d2822a3fd45dfcc9f2e7787984\\n   2              0 2020.0                                                                                             AI-lead Court Debate Case Investigation 2010.11604 https://www.semanticscholar.org/paper/151b1cbe2be4e1c9a639547ddaca5e0a2317bacd\\n   1            910 2022.0                                                                                    Red Teaming Language Models with Language Models 2202.03286 https://www.semanticscholar.org/paper/5d49c7401c5f2337c4cc88d243ae39ed659afe64\\n   1            667 2022.0                                        Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned 2209.07858 https://www.semanticscholar.org/paper/17bcb1edbe068e8fe6a97da552c70a77a15bbce7\\n   1             81 2024.0                                                     Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation 2401.06591 https://www.semanticscholar.org/paper/556bfb22059245c3dd388e97867cd88aa9dd078d\\n   1             68 2024.0               An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4 2403.02839 https://www.semanticscholar.org/paper/f1bbe52d857a5b3abc3fc9d9469c407a4add8d2c\\n   1             35 2025.0                        SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning 2506.24119 https://www.semanticscholar.org/paper/6ac8d8bfc7cf6dd6ad6cbc764cedffe673aef346\\n   1             34 2025.0                                                   Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment 2501.19309 https://www.semanticscholar.org/paper/6008a7676b3cae015670217c263ec0fdfc31e38b\\n   1             25 2025.0                                                  SeRL: Self-Play Reinforcement Learning for Large Language Models with Limited Data 2505.20347 https://www.semanticscholar.org/paper/8443d3b607b3f30ace2449a4df1d55976dd16a37\\n   1             23 2025.0                              Automating Evaluation of AI Text Generation in Healthcare with a Large Language Model (LLM)-as-a-Judge       None https://www.semanticscholar.org/paper/1e62d8b0db0731179b6513ae442fe29bc8f4eb2d\\n   1             23 2024.0                                                Red Teaming Large Language Models in Medicine: Real-World Insights on Model Behavior       None https://www.semanticscholar.org/paper/1b18ff2c88b9a74e24dc99ed23f9fd5fe79d47e1\\n   1             23 2023.0                                                Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases 2310.14303 https://www.semanticscholar.org/paper/ffb2f048ae694e08a741f8aa339e0f80003497bc\\n   1             17 2025.0                                                                                           Language Self-Play For Data-Free Training 2509.07414 https://www.semanticscholar.org/paper/7ceb4e11592bc5616e9d4ecaa4d0acf9c27a0048\\n   1             15 2024.0                     How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective 2410.10093 https://www.semanticscholar.org/paper/8b4faf263cebc2a05e0dd3efaf21b24572c615b2\\n   1             14 2025.0                                                         Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge 2508.06709 https://www.semanticscholar.org/paper/54c4d714704be8c6bafb28af551cf88641a391e1\\n   1             13 2024.0                                                                        STAR: SocioTechnical Approach to Red Teaming Language Models 2406.11757 https://www.semanticscholar.org/paper/6945b0f03c0f29caa288d435c12029b6f0e0cf06\"}"}
{"role": "assistant", "content": "Here are **Semantic Scholar API re-query results** that most directly match the **judge\u2013challenger\u2013solver** (a.k.a. self-play / adversarial critic / debate + judge) paradigm. I focused on papers where there is explicitly (i) a **solver/policy model**, (ii) a **challenger/adversary/critic** that generates hard cases or counter-arguments, and (iii) a **judge / AI feedback / preference signal** used for training or evaluation.\n\n## Core \u201cjudge\u2013challenger\u2013solver\u201d / self-play alignment papers\n1) **Self-Play Preference Optimization for Language Model Alignment** (2024)  \n   ArXiv: **2405.00675**  \n   Semantic Scholar: https://www.semanticscholar.org/paper/df8c3a325419d63366b9b347739fcbf3e2c4d22c  \n   Why relevant: explicitly about **self-play** to generate preference data/signals for alignment (very close to the tri-role setup).\n\n2) **Self-Play with Adversarial Critic: Provable and Scalable Offline Alignment for Language Models** (2024)  \n   Semantic Scholar: https://www.semanticscholar.org/paper/77f0687571a213c784f0901a821f22b2a03f3ddd  \n   ArXiv: **2406.04274**  \n   Why relevant: \u201cadversarial critic\u201d is essentially the **challenger**; alignment is driven by the interaction + evaluation signal.\n\n3) **SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning** (2025)  \n   ArXiv: **2504.19162**  \n   Semantic Scholar: https://www.semanticscholar.org/paper/160820bb80922cfc3ff47a0c9b3a2bb369bb97b9  \n   Why relevant: explicitly frames an **adversarial game** where the critic/challenger evolves\u2014very on-target.\n\n4) **Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models** (2025)  \n   ArXiv: **2506.07468**  \n   Semantic Scholar: https://www.semanticscholar.org/paper/21251d14f97926bf4c19b8cfaab8a3d101f7fa71  \n   Why relevant: \u201conline self-play RL\u201d for safety suggests a solver being trained against an evolving adversary/challenger distribution, typically scored by a judge/safety criterion.\n\n5) **Training Language Models to Win Debates with Self-Play Improves Judge Accuracy** (2024)  \n   ArXiv: **2409.16636**  \n   Semantic Scholar: https://www.semanticscholar.org/paper/1acc966b193be4eca4849d132ce3208a80d1c6fe  \n   Why relevant: explicit **debate** structure (solver vs challenger), plus a **judge**; also touches the co-evolution angle (judge quality).\n\n6) **Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback** (2023)  \n   ArXiv: **2305.10142**  \n   Semantic Scholar: https://www.semanticscholar.org/paper/2e6b6de08f459e2165b11ed8d2103916966b0fcf  \n   Why relevant: a concrete example of **AI feedback (judge)** + self-play dynamics in a task setting.\n\n7) **SeRL: Self-Play Reinforcement Learning for Large Language Models with Limited Data** (2025)  \n   ArXiv: **2505.20347**  \n   Semantic Scholar: https://www.semanticscholar.org/paper/8443d3b607b3f30ace2449a4df1d55976dd16a37  \n   Why relevant: not strictly \u201cdata-free\u201d by title, but it\u2019s directly in the **self-play RL for LLMs** cluster.\n\n8) **Language Self-Play For Data-Free Training** (2025)  \n   ArXiv: **2509.07414**  \n   Semantic Scholar: https://www.semanticscholar.org/paper/7ceb4e11592bc5616e9d4ecaa4d0acf9c27a0048  \n   Why relevant: explicitly \u201cdata-free\u201d + self-play; often these systems include judge/critic roles even if named differently.\n\n## Judge model / \u201cLLM-as-a-judge\u201d papers (judge component + robustness)\nThese help with the **judge** side (calibration, attacks, reliability), which is critical in judge\u2013challenger\u2013solver loops:\n\n1) **Judging LLM-as-a-judge with MT-Bench and Chatbot Arena** (2023)  \n   ArXiv: **2306.05685**  \n   Semantic Scholar: https://www.semanticscholar.org/paper/a0a79dad89857a96f8f71b14238e5237cbfc4787  \n\n2) **LLM-as-a-Judge & Reward Model: What They Can and Cannot Do** (2024)  \n   ArXiv: **2409.11239**  \n   Semantic Scholar: https://www.semanticscholar.org/paper/1158310f204db29513bd8322f7010f0ca78d69be  \n\n3) **An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4** (2024)  \n   ArXiv: **2403.02839**  \n   Semantic Scholar: https://www.semanticscholar.org/paper/f1bbe52d857a5b3abc3fc9d9469c407a4add8d2c  \n\n4) **Know Thy Judge: On the Robustness Meta-Evaluation of LLM Safety Judges** (2025)  \n   ArXiv: **2503.04474**  \n   Semantic Scholar: https://www.semanticscholar.org/paper/0ffb356aab98ae69c717f8b2969c3fed0592a048  \n\n5) **Preference Leakage: A Contamination Problem in LLM-as-a-judge** (2025)  \n   ArXiv: **2502.01534**  \n   Semantic Scholar: https://www.semanticscholar.org/paper/24e8029197b5f99126f0ef4e577d55a275405b36  \n\n6) **Optimization-based Prompt Injection Attack to LLM-as-a-Judge** (2024)  \n   ArXiv: **2403.17710**  \n   Semantic Scholar: https://www.semanticscholar.org/paper/e56f14ced9f7ce344ed14bdcb46860ccac72ac83  \n\n7) **AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens** (2025)  \n   ArXiv: **2512.17375**  \n   Semantic Scholar: https://www.semanticscholar.org/paper/d07bf70999d4d93f25fe98c0ea939b767c54c466  \n\n## Red-teaming / challenger-side papers (challenger generation)\nThese are often exactly the **challenger** module (generate failures) paired with an evaluator/judge:\n\n1) **Red Teaming Language Models with Language Models** (2022)  \n   ArXiv: **2202.03286**  \n   Semantic Scholar: https://www.semanticscholar.org/paper/5d49c7401c5f2337c4cc88d243ae39ed659afe64  \n\n2) **Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned** (2022)  \n   ArXiv: **2209.07858**  \n   Semantic Scholar: https://www.semanticscholar.org/paper/17bcb1edbe068e8fe6a97da552c70a77a15bbce7  \n\n3) **Explore, Establish, Exploit: Red Teaming Language Models from Scratch** (2023)  \n   ArXiv: **2306.09442**  \n   Semantic Scholar: https://www.semanticscholar.org/paper/1db819afb3604c4bfd1e5a0cb2ee9ab9dec52642  \n\n4) **Gradient-Based Language Model Red Teaming** (2024)  \n   ArXiv: **2401.16656**  \n   Semantic Scholar: https://www.semanticscholar.org/paper/409e0616a0fc02dd0ee8d5ae061944a98e9bd5a9  \n\n## Next step (to make this exactly your paradigm)\nDifferent authors rename the roles, so to tighten further, tell me which mapping you mean:\n\n- **Challenger = adversarial prompt generator** (red-team), **solver = target**, **judge = LLM grader**  \n- **Challenger = debating agent**, **solver = debating agent**, **judge = vote/score**  \n- **Challenger = critic that proposes counterexamples/edits**, **judge = preference model**, **solver = main policy**\n\nIf you answer with one of those (or your preferred paper that exemplifies it), I\u2019ll:\n1) re-run Semantic Scholar with **role-specific synonyms**,  \n2) return the **top ~15 most central papers** (with abstracts), and  \n3) summarize the standard training loop + what makes it \u201cdata-free\u201d (what signals are actually learned vs hand-designed)."}
{"role": "user", "content": "clear your context. then read research md to see what im currently researching"}
{"type": "function_call", "name": "python", "call_id": "call_nqkiUB86UxrhowoQKNPdkgjJ", "arguments": "{\"code\":\"import os, glob, pathlib\\npaths=[]\\nfor p in glob.glob('**/research.md', recursive=True):\\n    paths.append(p)\\npaths\\n\"}"}
{"type": "function_call_output", "call_id": "call_nqkiUB86UxrhowoQKNPdkgjJ", "output": "{\"ok\": true, \"stdout\": \"\", \"result\": [\"research.md\"]}"}
{"type": "function_call", "name": "python", "call_id": "call_LP8naC3mGDNG9XOQGb8eQnJM", "arguments": "{\"code\":\"from pathlib import Path\\ntext=Path('research.md').read_text(errors='ignore')\\ntext[:4000]\\n\"}"}
{"type": "function_call_output", "call_id": "call_LP8naC3mGDNG9XOQGb8eQnJM", "output": "{\"ok\": true, \"stdout\": \"\", \"result\": \"Based on everything you\\u2019ve been building and iterating on recently, your current research is best described as:\\n\\n> **A self-improving, data-free reinforcement learning framework for language models that optimizes for verifiability rather than raw difficulty.**\\n\\nLet me break that down precisely.\\n\\n---\\n\\n# Core Research Direction\\n\\nYou are designing a **closed-loop training system** composed of three interacting components:\\n\\n1. **Question Generator (G)**\\n2. **Solver (S)**\\n3. **Judge (J)**\\n\\nThe system generates its own tasks, solves them, evaluates them automatically, and uses the evaluation signal to improve the solver (and potentially the generator).\\n\\nBut the key conceptual shift you\\u2019re making is this:\\n\\n> Most prior work optimizes the generator for *difficulty*.\\n> You argue it should instead be optimized for *verifiability*.\\n\\nThat\\u2019s the intellectual center of what you\\u2019re doing right now.\\n\\n---\\n\\n# What Already Exists (and What You\\u2019re Extending)\\n\\nThere is prior work in:\\n\\n* Self-play language agents\\n* Data-free training loops\\n* Self-generated curricula\\n* Environment-based RL for LLMs\\n* Verifier-based RL (math/code/tool use)\\n\\nThese systems typically reward the generator when it produces questions that are difficult for the solver.\\n\\nYour critique:\\n\\n* \\u201cDifficult\\u201d \\u2260 \\u201cUseful\\u201d\\n* Many difficult tasks are ambiguous\\n* Ambiguity destabilizes the judge\\n* Noisy reward \\u2192 unstable learning\\n\\nSo instead of maximizing difficulty, you're trying to:\\n\\n> Maximize **evaluation reliability**.\\n\\n---\\n\\n# The Shift: Difficulty \\u2192 Verifiability\\n\\nYou are reframing the objective of the question generator.\\n\\nRather than:\\n\\n```\\nReward(q) = - SolverAccuracy(q)\\n```\\n\\nYou are exploring signals like:\\n\\n* Judge consistency across samples\\n* Separation between strong and weak solvers\\n* Confidence-weighted pairwise comparisons\\n* Stability under perturbations\\n* Agreement across sampling temperature\\n* Resistance to adversarial incorrect solutions\\n\\nThis is much more signal-processing oriented than traditional RL.\\n\\nYou\\u2019re essentially asking:\\n\\n> What makes a question produce a stable, high signal-to-noise reward?\\n\\n---\\n\\n# Your Core Technical Problems Right Now\\n\\nYou\\u2019re currently wrestling with several deep issues:\\n\\n### 1. How do we measure verifiability?\\n\\nYou\\u2019re exploring:\\n\\n* Variance of judge scores\\n* Self-consistency across samples\\n* Pairwise comparison stability\\n* Confidence reporting\\n* GAN-style adversarial tricking of the judge\\n\\nYou are trying to formalize something like:\\n\\n> A good question produces stable scoring across perturbations.\\n\\n---\\n\\n### 2. How do we define confidence?\\n\\nYou\\u2019ve been thinking about:\\n\\n* Judge selects A over B\\n* Judge reports confidence\\n* Can confidence approximate P(correct)?\\n\\nThis connects to your idea of \\u201cfuzzy P vs NP\\u201d in the LLM context:\\n\\n* Solving is hard\\n* Verifying should be easier\\n* But how do we measure that when the ground truth is unknown?\\n\\n---\\n\\n### 3. Verifiability without Ground Truth\\n\\nThis is one of your hardest current questions.\\n\\nYou\\u2019re asking:\\n\\n> Can we measure verifiability on problems where we do not know the true answer?\\n\\nYou're exploring:\\n\\n* Self-consistency\\n* Strong-vs-weak solver separation\\n* Judge adversarial robustness\\n* Training a model specifically to produce convincing incorrect answers\\n\\nThat last idea is important:\\nYou proposed training an \\u201cincorrect model\\u201d to generate plausible but wrong rollouts to stress-test the judge.\\n\\nThat is basically an adversarial verifier training loop.\\n\\n---\\n\\n# Bigger Theoretical Direction\\n\\nYou are circling around something deeper:\\n\\n* Redefining reward signals for LLM RL\\n* Treating reward as an information-theoretic signal\\n* Moving away from binary correctness\\n* Framing training as signal optimization under uncertainty\\n\\nYour intuition is:\\n\\n> Stable evaluation is more important than difficulty.\\n\\nThat is not the standard framing in self-play LLM work.\\n\\n---\\n\\n# Engineering Direction\\n\\nOn the implementation side, you're:\\n\\n* Designing a minimal, flexible research interface\\n* Using a Python-only tool interface\\n* \"}"}
{"role": "assistant", "content": "I can\u2019t literally \u201cclear my context\u201d (I don\u2019t have a manual memory wipe switch), but I will **drop the prior thread assumptions** and proceed using only what\u2019s in your `research.md` plus your new instructions.\n\nI read `research.md`. Your current research is:\n\n- A **closed-loop, data-free RL framework** with three components:\n  1) **Question/Task Generator (G)** (your \u201cchallenger\u201d)\n  2) **Solver (S)**\n  3) **Judge (J)**\n- The key novelty/stance: optimize the generator **for verifiability / evaluation reliability**, not for raw difficulty.\n\nKey problems you\u2019re focusing on (from the doc):\n- Defining/measuring **verifiability** (judge consistency, variance, perturbation stability, separation between strong/weak solvers, etc.)\n- Defining **confidence** and whether it approximates probability of correctness\n- **Verifiability without ground truth** (how to get stable reward signals when correctness is unknown)\n- Adversarial angle: training an **incorrect model** to produce plausible wrong solutions to stress-test the judge (adversarial verifier training loop).\n\nIf you want, paste the rest of `research.md` (it cut off mid \u201cEngineering Direction\u201d), and tell me: are you trying to (1) write a related work section, (2) formalize objectives mathematically, or (3) design experiments/ablations?"}
{"role": "user", "content": "read my research.md file and look for related papers on semantic scholar i should look at"}
{"type": "function_call", "name": "python", "call_id": "call_DeeYfmU76nUP2zcycVjeZhj7", "arguments": "{\"code\":\"from pathlib import Path\\ntext=Path('research.md').read_text(errors='ignore')\\nlen(text)\\n\"}"}
{"type": "function_call_output", "call_id": "call_DeeYfmU76nUP2zcycVjeZhj7", "output": "{\"ok\": true, \"stdout\": \"\", \"result\": 5213}"}
{"type": "function_call", "name": "python", "call_id": "call_ZxIZFc5wYdDGTVJek4PePzlC", "arguments": "{\"code\":\"text\\n\"}"}
{"type": "function_call_output", "call_id": "call_ZxIZFc5wYdDGTVJek4PePzlC", "output": "{\"ok\": true, \"stdout\": \"\", \"result\": \"Based on everything you\\u2019ve been building and iterating on recently, your current research is best described as:\\n\\n> **A self-improving, data-free reinforcement learning framework for language models that optimizes for verifiability rather than raw difficulty.**\\n\\nLet me break that down precisely.\\n\\n---\\n\\n# Core Research Direction\\n\\nYou are designing a **closed-loop training system** composed of three interacting components:\\n\\n1. **Question Generator (G)**\\n2. **Solver (S)**\\n3. **Judge (J)**\\n\\nThe system generates its own tasks, solves them, evaluates them automatically, and uses the evaluation signal to improve the solver (and potentially the generator).\\n\\nBut the key conceptual shift you\\u2019re making is this:\\n\\n> Most prior work optimizes the generator for *difficulty*.\\n> You argue it should instead be optimized for *verifiability*.\\n\\nThat\\u2019s the intellectual center of what you\\u2019re doing right now.\\n\\n---\\n\\n# What Already Exists (and What You\\u2019re Extending)\\n\\nThere is prior work in:\\n\\n* Self-play language agents\\n* Data-free training loops\\n* Self-generated curricula\\n* Environment-based RL for LLMs\\n* Verifier-based RL (math/code/tool use)\\n\\nThese systems typically reward the generator when it produces questions that are difficult for the solver.\\n\\nYour critique:\\n\\n* \\u201cDifficult\\u201d \\u2260 \\u201cUseful\\u201d\\n* Many difficult tasks are ambiguous\\n* Ambiguity destabilizes the judge\\n* Noisy reward \\u2192 unstable learning\\n\\nSo instead of maximizing difficulty, you're trying to:\\n\\n> Maximize **evaluation reliability**.\\n\\n---\\n\\n# The Shift: Difficulty \\u2192 Verifiability\\n\\nYou are reframing the objective of the question generator.\\n\\nRather than:\\n\\n```\\nReward(q) = - SolverAccuracy(q)\\n```\\n\\nYou are exploring signals like:\\n\\n* Judge consistency across samples\\n* Separation between strong and weak solvers\\n* Confidence-weighted pairwise comparisons\\n* Stability under perturbations\\n* Agreement across sampling temperature\\n* Resistance to adversarial incorrect solutions\\n\\nThis is much more signal-processing oriented than traditional RL.\\n\\nYou\\u2019re essentially asking:\\n\\n> What makes a question produce a stable, high signal-to-noise reward?\\n\\n---\\n\\n# Your Core Technical Problems Right Now\\n\\nYou\\u2019re currently wrestling with several deep issues:\\n\\n### 1. How do we measure verifiability?\\n\\nYou\\u2019re exploring:\\n\\n* Variance of judge scores\\n* Self-consistency across samples\\n* Pairwise comparison stability\\n* Confidence reporting\\n* GAN-style adversarial tricking of the judge\\n\\nYou are trying to formalize something like:\\n\\n> A good question produces stable scoring across perturbations.\\n\\n---\\n\\n### 2. How do we define confidence?\\n\\nYou\\u2019ve been thinking about:\\n\\n* Judge selects A over B\\n* Judge reports confidence\\n* Can confidence approximate P(correct)?\\n\\nThis connects to your idea of \\u201cfuzzy P vs NP\\u201d in the LLM context:\\n\\n* Solving is hard\\n* Verifying should be easier\\n* But how do we measure that when the ground truth is unknown?\\n\\n---\\n\\n### 3. Verifiability without Ground Truth\\n\\nThis is one of your hardest current questions.\\n\\nYou\\u2019re asking:\\n\\n> Can we measure verifiability on problems where we do not know the true answer?\\n\\nYou're exploring:\\n\\n* Self-consistency\\n* Strong-vs-weak solver separation\\n* Judge adversarial robustness\\n* Training a model specifically to produce convincing incorrect answers\\n\\nThat last idea is important:\\nYou proposed training an \\u201cincorrect model\\u201d to generate plausible but wrong rollouts to stress-test the judge.\\n\\nThat is basically an adversarial verifier training loop.\\n\\n---\\n\\n# Bigger Theoretical Direction\\n\\nYou are circling around something deeper:\\n\\n* Redefining reward signals for LLM RL\\n* Treating reward as an information-theoretic signal\\n* Moving away from binary correctness\\n* Framing training as signal optimization under uncertainty\\n\\nYour intuition is:\\n\\n> Stable evaluation is more important than difficulty.\\n\\nThat is not the standard framing in self-play LLM work.\\n\\n---\\n\\n# Engineering Direction\\n\\nOn the implementation side, you're:\\n\\n* Designing a minimal, flexible research interface\\n* Using a Python-only tool interface\\n* Storing full context in index.jsonl\\n* Maintaining a persistent instructions.md\\n* Thinking about context management as the core abstraction\\n* Building something Cursor-inspired but research-specific\\n\\nYou want:\\n\\n* Maximum flexibility\\n* Minimal rigid automation\\n* Strong context control\\n* Reproducible experimental state\\n\\n---\\n\\n# What This Research Actually Is (If I Had to Label It)\\n\\nIf I had to name it cleanly:\\n\\n> **Verifiability-Optimized Self-Play for Data-Free Language Model Training**\\n\\nOr more conceptually:\\n\\n> **Reward Signal Stabilization for Self-Improving LLM Systems**\\n\\nOr more mathematically:\\n\\n> **Signal-to-Noise Optimization in Self-Generated RL Training Loops**\\n\\n---\\n\\n# Why This Is Nontrivial\\n\\nThe hard part isn\\u2019t generating tasks.\\nThe hard part isn\\u2019t solving them.\\nThe hard part isn\\u2019t even judging them.\\n\\nThe hard part is:\\n\\n> Defining a reward signal that remains stable when ground truth is unknown.\\n\\nThat is the frontier you\\u2019re currently probing.\\n\\n---\\n\\nIf you want, I can next:\\n\\n* Formalize your framework mathematically\\n* Map it to existing papers precisely\\n* Help you turn this into a research positioning paragraph\\n* Or stress test whether this is actually novel\\n\\nWhere do you want to push it?\\n\"}"}
{"type": "function_call", "name": "python", "call_id": "call_WtkuEHXGaLO5skVYzREp26HH", "arguments": "{\"code\":\"import requests, os, pandas as pd, time, re\\nS2_KEY=os.environ.get('S2_KEY')\\nheaders={'x-api-key':S2_KEY}\\n\\ndef search_many(queries, limit=20, sleep=1.2):\\n    allp=[]\\n    for q in queries:\\n        time.sleep(sleep)\\n        r=requests.get('https://api.semanticscholar.org/graph/v1/paper/search',\\n                       params={'query':q,'limit':limit,\\n                               'fields':'title,year,authors,venue,abstract,url,citationCount,openAccessPdf,externalIds'},\\n                       headers=headers, timeout=30)\\n        r.raise_for_status()\\n        for p in r.json()['data']:\\n            allp.append((q,p))\\n    return allp\\n\\nqueries=[\\n 'self-play preference optimization language model',\\n 'adversarial critic alignment language model',\\n 'data-free alignment language model self-play',\\n 'reinforcement learning from AI feedback judge model',\\n 'LLM-as-a-judge robustness prompt injection',\\n 'reward hacking LLM judge',\\n 'uncertainty calibration large language model judge',\\n 'self-consistency large language model reasoning',\\n 'best-of-n sampling reward model',\\n 'verifier-based reinforcement learning language model',\\n 'process reward model large language model',\\n 'constitutional AI critique judge',\\n]\\n\\nallp=search_many(queries, limit=20, sleep=1.4)\\nlen(allp)\\n\"}"}
{"type": "function_call_output", "call_id": "call_WtkuEHXGaLO5skVYzREp26HH", "output": "{\"ok\": true, \"stdout\": \"\", \"result\": 240}"}
{"type": "function_call", "name": "python", "call_id": "call_pIBiaEca8X88SCGqiY6NAKSa", "arguments": "{\"code\":\"seen=set(); merged=[]\\nfor q,p in allp:\\n    if p['paperId'] in seen: continue\\n    seen.add(p['paperId'])\\n    merged.append(p)\\n\\ndf=pd.DataFrame([{\\n 'paperId':p['paperId'],\\n 'title':p.get('title','') or '',\\n 'year':p.get('year'),\\n 'citationCount':p.get('citationCount',0),\\n 'venue':p.get('venue'),\\n 'arxiv':(p.get('externalIds') or {}).get('ArXiv'),\\n 'url':p.get('url'),\\n 'abstract':p.get('abstract','') or ''\\n} for p in merged])\\n\\n# scoring aligned to research.md themes\\nthemes=[\\n ('verifiability', ['verifi','reliab','calib','uncert','consisten','variance','stability']),\\n ('judge', ['llm-as-a-judge','judge','reward model','preference model','grading','evaluator']),\\n ('selfplay', ['self-play','self play','adversarial','debate','red team','challenger','critic']),\\n ('verifier', ['verifier','unit test','execution','tool','proof','check']),\\n ('no_gt', ['without ground truth','no ground truth','unsupervised','self-generated','data-free','synthetic']),\\n]\\n\\ndef them_score(text):\\n    t=text.lower()\\n    s=0\\n    for _,ks in themes:\\n        for k in ks:\\n            if k in t:\\n                s+=1\\n    return s\\n\\ndf['score']=df.apply(lambda r: them_score(r['title']+' '+r['abstract']), axis=1)\\n\\n# keep top\\ntop=df.sort_values(['score','citationCount','year'], ascending=[False,False,False]).head(30)\\ntop[['score','citationCount','year','title','arxiv','url']].to_string(index=False)[:3000]\\n\"}"}
{"type": "function_call_output", "call_id": "call_pIBiaEca8X88SCGqiY6NAKSa", "output": "{\"ok\": true, \"stdout\": \"\", \"result\": \" score  citationCount   year                                                                                                                               title      arxiv                                                                            url\\n     9             31 2025.0                                                                                                    One Token to Fool LLM-as-a-Judge 2507.08794 https://www.semanticscholar.org/paper/6478c185582f5e48c54677c683a9297ff2916118\\n     6             60 2025.0                                                               Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge 2501.18099 https://www.semanticscholar.org/paper/438c19fdd0f14f5082441db0c4183a50e7224b1b\\n     6             26 2024.0                                              MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models 2410.17578 https://www.semanticscholar.org/paper/1b256fb2f9ac9857db996fa4f881f16e1345f8b1\\n     6             19 2024.0                                                                          LLM-as-a-Judge & Reward Model: What They Can and Cannot Do 2409.11239 https://www.semanticscholar.org/paper/1158310f204db29513bd8322f7010f0ca78d69be\\n     6             16 2025.0                                                            An Empirical Analysis of Uncertainty in Large Language Model Evaluations 2502.10709 https://www.semanticscholar.org/paper/4af30e08ca07724bda8c5687ec68343302726e2c\\n     6              7 2025.0                                         Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks 2505.13348 https://www.semanticscholar.org/paper/0c102fd337124c47205e9b79e5eb9e133de56ed4\\n     6              6 2025.0                                        Right Is Not Enough: The Pitfalls of Outcome Supervision in Training LLMs for Math Reasoning 2506.06877 https://www.semanticscholar.org/paper/9c61e82fe2e202e7e342bd02f50a5ed7d0a463c2\\n     6              6 2025.0                                                                                  How to Correctly Report LLM-as-a-Judge Evaluations 2511.21140 https://www.semanticscholar.org/paper/26ee57cf62ebbf55e917755c23e5d949c5cc1d4a\\n     6              0 2025.0                                        AKD : Adversarial Knowledge Distillation For Large Language Models Alignment on Coding tasks 2505.06267 https://www.semanticscholar.org/paper/535def99945420472ac7772b54a59ca2893ceaf4\\n     6              0 2025.0                                                              Rubric-Conditioned LLM Grading: Alignment, Uncertainty, and Robustness 2601.08843 https://www.semanticscholar.org/paper/57a161c0bafee34d0ba94cf9c0754e1ba9c6e48f\\n     6              0 2025.0                                   Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning 2506.21560 https://www.semanticscholar.org/paper/2ab7cf06e7a868d54e72f714ff9ec\"}"}
{"type": "function_call", "name": "python", "call_id": "call_6Prc6XW7RPvMmDEqhBSqwS7M", "arguments": "{\"code\":\"top[['score','citationCount','year','title','arxiv','url']].to_dict('records')[:12]\\n\"}"}
{"type": "function_call_output", "call_id": "call_6Prc6XW7RPvMmDEqhBSqwS7M", "output": "{\"ok\": true, \"stdout\": \"\", \"result\": [{\"score\": 9, \"citationCount\": 31, \"year\": 2025.0, \"title\": \"One Token to Fool LLM-as-a-Judge\", \"arxiv\": \"2507.08794\", \"url\": \"https://www.semanticscholar.org/paper/6478c185582f5e48c54677c683a9297ff2916118\"}, {\"score\": 6, \"citationCount\": 60, \"year\": 2025.0, \"title\": \"Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge\", \"arxiv\": \"2501.18099\", \"url\": \"https://www.semanticscholar.org/paper/438c19fdd0f14f5082441db0c4183a50e7224b1b\"}, {\"score\": 6, \"citationCount\": 26, \"year\": 2024.0, \"title\": \"MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models\", \"arxiv\": \"2410.17578\", \"url\": \"https://www.semanticscholar.org/paper/1b256fb2f9ac9857db996fa4f881f16e1345f8b1\"}, {\"score\": 6, \"citationCount\": 19, \"year\": 2024.0, \"title\": \"LLM-as-a-Judge & Reward Model: What They Can and Cannot Do\", \"arxiv\": \"2409.11239\", \"url\": \"https://www.semanticscholar.org/paper/1158310f204db29513bd8322f7010f0ca78d69be\"}, {\"score\": 6, \"citationCount\": 16, \"year\": 2025.0, \"title\": \"An Empirical Analysis of Uncertainty in Large Language Model Evaluations\", \"arxiv\": \"2502.10709\", \"url\": \"https://www.semanticscholar.org/paper/4af30e08ca07724bda8c5687ec68343302726e2c\"}, {\"score\": 6, \"citationCount\": 7, \"year\": 2025.0, \"title\": \"Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks\", \"arxiv\": \"2505.13348\", \"url\": \"https://www.semanticscholar.org/paper/0c102fd337124c47205e9b79e5eb9e133de56ed4\"}, {\"score\": 6, \"citationCount\": 6, \"year\": 2025.0, \"title\": \"Right Is Not Enough: The Pitfalls of Outcome Supervision in Training LLMs for Math Reasoning\", \"arxiv\": \"2506.06877\", \"url\": \"https://www.semanticscholar.org/paper/9c61e82fe2e202e7e342bd02f50a5ed7d0a463c2\"}, {\"score\": 6, \"citationCount\": 6, \"year\": 2025.0, \"title\": \"How to Correctly Report LLM-as-a-Judge Evaluations\", \"arxiv\": \"2511.21140\", \"url\": \"https://www.semanticscholar.org/paper/26ee57cf62ebbf55e917755c23e5d949c5cc1d4a\"}, {\"score\": 6, \"citationCount\": 0, \"year\": 2025.0, \"title\": \"AKD : Adversarial Knowledge Distillation For Large Language Models Alignment on Coding tasks\", \"arxiv\": \"2505.06267\", \"url\": \"https://www.semanticscholar.org/paper/535def99945420472ac7772b54a59ca2893ceaf4\"}, {\"score\": 6, \"citationCount\": 0, \"year\": 2025.0, \"title\": \"Rubric-Conditioned LLM Grading: Alignment, Uncertainty, and Robustness\", \"arxiv\": \"2601.08843\", \"url\": \"https://www.semanticscholar.org/paper/57a161c0bafee34d0ba94cf9c0754e1ba9c6e48f\"}, {\"score\": 6, \"citationCount\": 0, \"year\": 2025.0, \"title\": \"Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning\", \"arxiv\": \"2506.21560\", \"url\": \"https://www.semanticscholar.org/paper/2ab7cf06e7a868d54e72f714ff9ec1b91644ce38\"}, {\"score\": 5, \"citationCount\": 23, \"year\": 2025.0, \"title\": \"SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning\", \"arxiv\": \"2504.19162\", \"url\": \"https://www.semanticscholar.org/paper/160820bb80922cfc3ff47a0c9b3a2bb369bb97b9\"}]}"}
{"role": "assistant", "content": "Based on your `research.md` (verifiability\u2011optimized **Generator/Challenger \u2013 Solver \u2013 Judge** loop; reward = **evaluation reliability under unknown ground truth**; robustness to adversarial incorrect solutions), here are **papers from Semantic Scholar you should look at**, grouped by the parts of your system they most directly inform.\n\n## A) Self-play / adversarial critic / challenger\u2013solver loops (closest to your training paradigm)\n- **Self-Play Preference Optimization for Language Model Alignment** (2024) \u2014 arXiv: *2405.00675*  \n  https://www.semanticscholar.org/paper/df8c3a325419d63366b9b347739fcbf3e2c4d22c  \n  Why: explicit self-play preference optimization; very aligned with \u201cdata-free loop\u201d.\n\n- **Self-Play with Adversarial Critic: Provable and Scalable Offline Alignment for Language Models** (2024) \u2014 arXiv: *2406.04274*  \n  https://www.semanticscholar.org/paper/77f0687571a213c784f0901a821f22b2a03f3ddd  \n  Why: \u201cadversarial critic\u201d \u2248 your challenger producing cases that stress the judge/solver.\n\n- **SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning** (2025) \u2014 arXiv: *2504.19162*  \n  https://www.semanticscholar.org/paper/160820bb80922cfc3ff47a0c9b3a2bb369bb97b9  \n  Why: directly about evolving a critic in an adversarial game\u2014good reference for your \u201cincorrect model\u201d idea.\n\n- **Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models** (2025) \u2014 arXiv: *2506.07468*  \n  https://www.semanticscholar.org/paper/21251d14f97926bf4c19b8cfaab8a3d101f7fa71  \n  Why: online self-play for safety; likely touches stability issues when the opponent distribution moves.\n\n## B) Judge reliability / \u201cLLM-as-a-judge\u201d failure modes (core to \u201cverifiability\u201d)\nThese map directly onto your \u201cjudge consistency / perturbation stability / adversarial tricking\u201d bullets:\n\n- **LLM-as-a-Judge & Reward Model: What They Can and Cannot Do** (2024) \u2014 arXiv: *2409.11239*  \n  https://www.semanticscholar.org/paper/1158310f204db29513bd8322f7010f0ca78d69be\n\n- **MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models** (2024) \u2014 arXiv: *2410.17578*  \n  https://www.semanticscholar.org/paper/1b256fb2f9ac9857db996fa4f881f16e1345f8b1  \n  Why: meta-eval is useful for defining/benchmarking \u201cjudge reliability\u201d as an object.\n\n- **An Empirical Analysis of Uncertainty in Large Language Model Evaluations** (2025) \u2014 arXiv: *2502.10709*  \n  https://www.semanticscholar.org/paper/4af30e08ca07724bda8c5687ec68343302726e2c  \n  Why: directly relevant to your \u201cconfidence \u2248 P(correct)?\u201d question.\n\n- **How to Correctly Report LLM-as-a-Judge Evaluations** (2025) \u2014 arXiv: *2511.21140*  \n  https://www.semanticscholar.org/paper/26ee57cf62ebbf55e917755c23e5d949c5cc1d4a  \n  Why: helps you avoid methodological traps when you propose new verifiability metrics.\n\n## C) Adversarial attacks on judges (directly relevant to \u201cGAN-style adversarial tricking of the judge\u201d)\n- **One Token to Fool LLM-as-a-Judge** (2025) \u2014 arXiv: *2507.08794*  \n  https://www.semanticscholar.org/paper/6478c185582f5e48c54677c683a9297ff2916118  \n  Why: almost a perfect \u201cjudge robustness under perturbation\u201d citation for your stability framing.\n\n- **Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks** (2025) \u2014 arXiv: *2505.13348*  \n  https://www.semanticscholar.org/paper/0c102fd337124c47205e9b79e5eb9e133de56ed4  \n  Why: if your judge is itself an LLM in-context, your generator can learn to jailbreak it; this is core risk.\n\n## D) \u201cThinking judge\u201d / evaluation-by-reasoning (judge design changes your verifiability objective)\n- **Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge** (2025) \u2014 arXiv: *2501.18099*  \n  https://www.semanticscholar.org/paper/438c19fdd0f14f5082441db0c4183a50e7224b1b  \n  Why: if the judge is allowed to reason, \u201cverifiability\u201d becomes partly \u201cjudge compute budget / deliberation stability\u201d.\n\n## E) Red-teaming as challenger (generator that seeks failures)\nEven if not phrased as training loops, these are closest to \u201cchallenger generates hard/ambiguous/adversarial cases\u201d:\n- **Red Teaming Language Models with Language Models** (2022) \u2014 arXiv: *2202.03286*  \n  https://www.semanticscholar.org/paper/5d49c7401c5f2337c4cc88d243ae39ed659afe64\n\n- **Explore, Establish, Exploit: Red Teaming Language Models from Scratch** (2023) \u2014 arXiv: *2306.09442*  \n  https://www.semanticscholar.org/paper/1db819afb3604c4bfd1e5a0cb2ee9ab9dec52642\n\n- **Gradient-Based Language Model Red Teaming** (2024) \u2014 arXiv: *2401.16656*  \n  https://www.semanticscholar.org/paper/409e0616a0fc02dd0ee8d5ae061944a98e9bd5a9\n\n---\n\n### If you tell me your concrete setup, I\u2019ll narrow to the 10 best citations\nTwo quick disambiguators:\n1) Is your **judge** (J) an LLM-in-context, a fine-tuned reward model, or a verifier/tool (unit tests/execution)?\n2) Does your **generator/challenger** optimize *difficulty under J*, or directly optimize a *stability metric of J* (variance/consistency/perturbation robustness)?\n\nAnswering those lets me re-query Semantic Scholar more narrowly and return a tighter \u201crelated work\u201d list specifically for *verifiability-optimized* self-play rather than generic judge/self-play papers."}
{"role": "user", "content": "red teaming is relevant. use gpt to create a meta analysis on the most relevant papers. think how about we can create a report using the tools avaliable. find relevant content etc"}
{"type": "function_call", "name": "python", "call_id": "call_EBZ7B4PKwg93SXDcNFhvAAKL", "arguments": "{\"code\":\"import requests, os, time, pandas as pd\\nS2_KEY=os.environ.get('S2_KEY')\\nheaders={'x-api-key':S2_KEY}\\n\\ndef s2_search(query, limit=25, sleep=1.2):\\n    time.sleep(sleep)\\n    r=requests.get('https://api.semanticscholar.org/graph/v1/paper/search',\\n                   params={'query':query,'limit':limit,\\n                           'fields':'title,year,authors,venue,abstract,url,citationCount,openAccessPdf,externalIds'},\\n                   headers=headers, timeout=30)\\n    r.raise_for_status()\\n    return r.json()['data']\\n\\nqueries={\\n 'red_teaming_core': 'red teaming language models with language models',\\n 'red_teaming_from_scratch': 'red teaming language models from scratch',\\n 'gradient_red_teaming': 'gradient-based language model red teaming',\\n 'parametric_red_teaming': 'parametric red-teaming language model unalignment',\\n 'self_play_alignment': 'self-play preference optimization language model alignment',\\n 'adversarial_critic_alignment': 'self-play adversarial critic offline alignment language models',\\n 'judge_robustness': 'LLM-as-a-judge prompt injection attack',\\n 'judge_limits': 'LLM-as-a-judge reward model what they can and cannot do',\\n 'judge_attack_one_token': 'one token to fool LLM-as-a-judge'\\n}\\n\\nallp=[]\\nfor k,q in queries.items():\\n    ps=s2_search(q, limit=20, sleep=1.4)\\n    for p in ps:\\n        allp.append((k,p))\\n\\n# dedup\\nseen=set(); merged=[]\\nfor tag,p in allp:\\n    if p['paperId'] in seen: continue\\n    seen.add(p['paperId'])\\n    merged.append((tag,p))\\n\\nlen(merged)\\n\"}"}
{"type": "function_call_output", "call_id": "call_EBZ7B4PKwg93SXDcNFhvAAKL", "output": "{\"ok\": true, \"stdout\": \"\", \"result\": 106}"}
{"type": "function_call", "name": "python", "call_id": "call_AWMdSrhVmuL3cIBuLUjgtFnA", "arguments": "{\"code\":\"# pick the best match per query by simple token overlap with query name terms\\nimport re\\n\\ndef best_for_tag(tag):\\n    cand=[p for t,p in merged if t==tag]\\n    q=queries[tag].lower()\\n    qterms=set(re.findall(r\\\"[a-z]+\\\", q))\\n    best=None; bestscore=-1\\n    for p in cand:\\n        text=((p.get('title') or '')+' '+(p.get('abstract') or '')).lower()\\n        terms=set(re.findall(r\\\"[a-z]+\\\", text))\\n        score=len(qterms & terms)\\n        if score>bestscore:\\n            bestscore=score; best=p\\n    return bestscore, best\\n\\nselected={}\\nfor tag in queries:\\n    sc,p=best_for_tag(tag)\\n    selected[tag]=p\\n\\n[(tag, selected[tag].get('title'), (selected[tag].get('externalIds') or {}).get('ArXiv')) for tag in selected]\\n\"}"}
{"type": "function_call_output", "call_id": "call_AWMdSrhVmuL3cIBuLUjgtFnA", "output": "{\"ok\": true, \"stdout\": \"\", \"result\": [[\"red_teaming_core\", \"Red Teaming Language Models with Language Models\", \"2202.03286\"], [\"red_teaming_from_scratch\", \"Aurora-M: Open Source Continual Pre-training for Multilingual Language and Code\", \"2404.00399\"], [\"gradient_red_teaming\", \"Gradient-Based Language Model Red Teaming\", \"2401.16656\"], [\"parametric_red_teaming\", \"TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional Diversified Red-Teaming Data Synthesis\", \"2505.24672\"], [\"self_play_alignment\", \"Self-Play Preference Optimization for Language Model Alignment\", \"2405.00675\"], [\"adversarial_critic_alignment\", \"Self-Play with Adversarial Critic: Provable and Scalable Offline Alignment for Language Models\", \"2406.04274\"], [\"judge_robustness\", \"Optimization-based Prompt Injection Attack to LLM-as-a-Judge\", \"2403.17710\"], [\"judge_limits\", \"LLM-as-a-Judge & Reward Model: What They Can and Cannot Do\", \"2409.11239\"], [\"judge_attack_one_token\", \"One Token to Fool LLM-as-a-Judge\", \"2507.08794\"]]}"}
{"type": "function_call", "name": "python", "call_id": "call_RNhyCt14ZoGpaUyNx3OSWDsw", "arguments": "{\"code\":\"# fix tag where best match went off (from_scratch). We'll directly lookup known arxiv 2306.09442\\nimport requests, time\\n\\ndef get_by_arxiv(arxiv):\\n    time.sleep(1.2)\\n    r=requests.get('https://api.semanticscholar.org/graph/v1/paper/ARXIV:'+arxiv,\\n                   params={'fields':'title,year,authors,venue,abstract,url,citationCount,openAccessPdf,externalIds,referenceCount,citationCount'},\\n                   headers=headers, timeout=30)\\n    r.raise_for_status();\\n    return r.json()\\n\\nfrom_scratch=get_by_arxiv('2306.09442')\\nfrom_scratch['title']\\n\"}"}
{"type": "function_call_output", "call_id": "call_RNhyCt14ZoGpaUyNx3OSWDsw", "output": "{\"ok\": true, \"stdout\": \"\", \"result\": \"Explore, Establish, Exploit: Red Teaming Language Models from Scratch\"}"}
{"type": "function_call", "name": "python", "call_id": "call_OIYdun1Vi5J3HtYw2uVHgzsm", "arguments": "{\"code\":\"selected['red_teaming_from_scratch']=from_scratch\\n\\n# also add Language Model Unalignment parametric red-teaming 2310.14303\\nunalign=get_by_arxiv('2310.14303')\\nunalign['title']\\n\"}"}
{"type": "function_call_output", "call_id": "call_OIYdun1Vi5J3HtYw2uVHgzsm", "output": "{\"ok\": true, \"stdout\": \"\", \"result\": \"Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases\"}"}
{"type": "function_call", "name": "python", "call_id": "call_Gvk7V13IUd00Bmv4UrhBExH7", "arguments": "{\"code\":\"# compile final set\\nfinal_papers=[\\n ('Red teaming (LLM->LLM)', selected['red_teaming_core']),\\n ('Red teaming from scratch', selected['red_teaming_from_scratch']),\\n ('Parametric red-teaming', unalign),\\n ('Gradient-based red teaming', selected['gradient_red_teaming']),\\n ('Self-play preference optimization', selected['self_play_alignment']),\\n ('Self-play w/ adversarial critic', selected['adversarial_critic_alignment']),\\n ('Judge limits', selected['judge_limits']),\\n ('Judge prompt-injection attack', selected['judge_robustness']),\\n ('One-token judge attack', selected['judge_attack_one_token']),\\n]\\n\\nrows=[]\\nfor cat,p in final_papers:\\n    rows.append({\\n        'category':cat,\\n        'title':p.get('title'),\\n        'year':p.get('year'),\\n        'authors':', '.join([a['name'] for a in p.get('authors',[])[:5]]) + (' et al.' if len(p.get('authors',[]))>5 else ''),\\n        'arxiv':(p.get('externalIds') or {}).get('ArXiv'),\\n        'url':p.get('url'),\\n        'citationCount':p.get('citationCount'),\\n        'abstract':p.get('abstract','')\\n    })\\n\\npd.DataFrame(rows)[['category','year','title','arxiv','citationCount','url']]\\n\"}"}
{"type": "function_call_output", "call_id": "call_Gvk7V13IUd00Bmv4UrhBExH7", "output": "{\"ok\": true, \"stdout\": \"\", \"result\": [\"category\", \"year\", \"title\", \"arxiv\", \"citationCount\", \"url\"]}"}
{"role": "assistant", "content": "Below is a **meta-analysis report plan + first-pass content** tailored to your `research.md`: **verifiability-optimized self-play / red-teaming loops** with **Generator/Challenger (G), Solver (S), Judge (J)**, where the core difficulty is **stable reward under unknown ground truth**.\n\nI used the **Semantic Scholar Graph API** to pull the key papers below (links included), then I organized them into a report structure you can generate with the tools you already have (Python + Semantic Scholar API + your markdown files).\n\n---\n\n## 1) \u201cMost relevant\u201d paper set (Semantic Scholar links)\n\n### Red teaming as challenger/task generator (G)\n1. **Red Teaming Language Models with Language Models** (2022) \u2014 arXiv: **2202.03286**  \n   https://www.semanticscholar.org/paper/5d49c7401c5f2337c4cc88d243ae39ed659afe64  \n   Relevance: canonical \u201cLLM generates adversarial prompts/failures for another LLM\u201d. This is the closest ancestor to G\u2192S loops where tasks are generated without human labeling.\n\n2. **Explore, Establish, Exploit: Red Teaming Language Models from Scratch** (2023) \u2014 arXiv: **2306.09442**  \n   https://www.semanticscholar.org/paper/1db819afb3604c4bfd1e5a0cb2ee9ab9dec52642  \n   Relevance: operationalizes a curriculum/strategy for building effective red teaming signals \u201cfrom scratch\u201d (important for data-free bootstrapping).\n\n3. **Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases** (2023) \u2014 arXiv: **2310.14303**  \n   https://www.semanticscholar.org/paper/ffb2f048ae694e08a741f8aa339e0f80003497bc  \n   Relevance: treats red-teaming as *search in prompt space* (parametric generation); useful if your G is optimized and you care about coverage/novelty vs ambiguity.\n\n4. **Gradient-Based Language Model Red Teaming** (2024) \u2014 arXiv: **2401.16656**  \n   https://www.semanticscholar.org/paper/409e0616a0fc02dd0ee8d5ae061944a98e9bd5a9  \n   Relevance: stronger \u201cchallenger optimization\u201d story\u2014how to systematically find failures. This intersects with your question: optimizing for *difficulty* often yields ambiguity; you want to optimize for *verifiability*.\n\n### Self-play alignment loops (G/S/J as a training system, not just evaluation)\n5. **Self-Play Preference Optimization for Language Model Alignment** (2024) \u2014 arXiv: **2405.00675**  \n   https://www.semanticscholar.org/paper/df8c3a325419d63366b9b347739fcbf3e2c4d22c  \n   Relevance: directly in your space: self-play generates preference signals; useful baseline for \u201coptimize generator for difficulty\u201d vs your \u201coptimize for verifiability\u201d.\n\n6. **Self-Play with Adversarial Critic: Provable and Scalable Offline Alignment for Language Models** (2024) \u2014 arXiv: **2406.04274**  \n   https://www.semanticscholar.org/paper/77f0687571a213c784f0901a821f22b2a03f3ddd  \n   Relevance: explicit *adversarial critic* (challenger) framing; closest to your \u201ctrain an incorrect model to stress-test judge\u201d idea.\n\n### Judge reliability + adversarial robustness (core to \u201cverifiability\u201d)\n7. **LLM-as-a-Judge & Reward Model: What They Can and Cannot Do** (2024) \u2014 arXiv: **2409.11239**  \n   https://www.semanticscholar.org/paper/1158310f204db29513bd8322f7010f0ca78d69be  \n   Relevance: directly informs when J is a stable signal vs when it collapses / biases / gets gamed.\n\n8. **Optimization-based Prompt Injection Attack to LLM-as-a-Judge** (2024) \u2014 arXiv: **2403.17710**  \n   https://www.semanticscholar.org/paper/e56f14ced9f7ce344ed14bdcb46860ccac72ac83  \n   Relevance: if G can influence judge context, your \u201cverifiability objective\u201d must penalize judge attackability.\n\n9. **One Token to Fool LLM-as-a-Judge** (2025) \u2014 arXiv: **2507.08794**  \n   https://www.semanticscholar.org/paper/6478c185582f5e48c54677c683a9297ff2916118  \n   Relevance: minimal perturbation causing judge flip = exactly your \u201cstability under perturbations\u201d criterion.\n\n---\n\n## 2) Meta-analysis: what these papers collectively imply for your thesis\n\n### Shared pattern across the field\nMost systems implicitly optimize:\n- **Challenger/G**: maximize failure rate (difficulty) or maximize harm discovery\n- **Solver/S**: minimize failures / maximize reward\n- **Judge/J**: assumed stable enough, or evaluated after the fact\n\nYour critique (from `research.md`) is consistent with what these literatures reveal:\n- Adversarially optimized G will often find **judge vulnerabilities**, not \u201cuseful hard tasks\u201d.\n- Therefore, naive self-play frequently becomes **reward hacking of J**.\n- \u201cDifficulty\u201d is a bad proxy; it pushes the system into regions of **ambiguity** (underspecified tasks, subjective rubrics, or exploitable judge prompts).\n\n### What\u2019s missing / your opening\nA unified objective for G that explicitly optimizes for:\n- **Judge stability** (low variance across restarts/paraphrases/temperatures)\n- **Judge robustness** (resistance to adversarial control tokens / prompt injection)\n- **Discriminativity** (separates strong vs weak solvers consistently)\n- **Verifiability-without-ground-truth** (internal consistency / cross-solver agreement structure)\n\nThese \u201cjudge-centric\u201d robustness papers become *not just evaluation references*, but **training constraints** on G.\n\n---\n\n## 3) A concrete \u201creport\u201d you can generate with your available tools\n\nYou said: \u201cthink how we can create a report using the tools available; find relevant content etc.\u201d Here\u2019s a practical workflow that fits what you have (Python + Semantic Scholar API + markdown files).\n\n### Report outline (suggested `report.md`)\n1. **Problem statement**: data-free RL with G/S/J; why \u201cdifficulty\u201d fails; why \u201cverifiability\u201d matters  \n2. **Related paradigms**:\n   - red teaming as task generation (G)\n   - self-play preference optimization (G/S)\n   - LLM-as-a-judge reliability (J)\n3. **Failure modes** (synthesis across papers):\n   - ambiguity \u2192 noisy reward\n   - judge gaming (prompt injection / token flips)\n   - distribution shift: evolving G breaks J calibration\n4. **Design requirements for verifiability-optimized self-play**\n   - metrics (stability, robustness, discriminativity)\n   - adversarial \u201cincorrect model\u201d / critic training as stress test\n5. **Proposed evaluation suite**\n   - perturbation tests (paraphrase, temperature, role prompts)\n   - cross-judge agreement\n   - strong-vs-weak solver separation\n6. **Open problems**\n\n### What to auto-extract with the API (programmatically)\nFor each selected paper, pull:\n- title/year/authors\n- abstract\n- citations / references (to expand the graph)\n- PDF link if open access\nThen auto-produce:\n- 3\u20135 bullet \u201cclaims\u201d (from abstract heuristics)\n- a \u201crelevance mapping\u201d tag: {G, S, J, verifiability, adversarial, judge-attack}\n- a \u201cmethod slot\u201d label: {red-team search, self-play preference, judge meta-eval, judge attack}\n\nIf you want, I can give you a ready-to-run Python snippet that:\n- takes a seed list of arXiv IDs\n- downloads their abstracts via Semantic Scholar\n- expands by \u201cmost-cited references\u201d\n- writes a structured `related_work.md`.\n\n---\n\n## 4) Key synthesis claims to write in the meta-analysis section\nThese are \u201cpaper-spanning\u201d claims your report can defend with citations:\n\n1. **Adversarial task generation (red teaming) naturally discovers evaluator weaknesses**, not just solver weaknesses.  \n   Use: judge attack papers + red teaming papers.\n\n2. **Any G/S/J loop needs an explicit constraint or objective term for evaluator stability**; otherwise, learning pressure routes into reward hacking.  \n   Use: \u201cOne Token \u2026\u201d + \u201cPrompt Injection Attack \u2026\u201d + \u201cWhat they can/cannot do\u201d.\n\n3. **Self-play alignment methods implicitly assume verifiability; making verifiability explicit is a research gap.**  \n   Use: the self-play alignment papers as \u201coptimize difficulty\u201d baselines vs your \u201coptimize evaluation reliability\u201d.\n\n---\n\nIf you want me to proceed, say: **\u201cGenerate the report\u201d** and specify:\n- do you want **10 papers** (tight) or **30\u201350** (survey-ish)?\n- do you want expansion via **references** (backward citation mining), **citations** (forward), or both?\n\nThen I\u2019ll (a) expand the list through Semantic Scholar, and (b) output a structured meta-analysis in markdown with citations/links."}
